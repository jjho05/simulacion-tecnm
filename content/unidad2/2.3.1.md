# 2.3.1 Características del Método de Monte Carlo

El Método de Monte Carlo es una de las herramientas más poderosas en la simulación moderna. Su nombre, inspirado en el famoso casino de Mónaco, refleja su naturaleza probabilística: usar la aleatoriedad controlada para resolver problemas determinísticos o estocásticos complejos.

---

## Fundamentos Matemáticos

### La Ley de los Grandes Números (LGN)

El pilar fundamental del Método de Monte Carlo es la **Ley de los Grandes Números**, que establece:

$$\lim_{n \to \infty} \frac{1}{n}\sum_{i=1}^{n} X_i = E[X] \quad \text{(casi seguramente)}$$

**Interpretación:** Si repetimos un experimento aleatorio infinitas veces, el promedio de los resultados convergerá al valor esperado teórico.

**Ejemplo Intuitivo:**

Si lanzas una moneda justa:
- 10 lanzamientos: Podrías obtener 7 caras (70%)
- 100 lanzamientos: Probablemente ~55 caras (55%)
- 10,000 lanzamientos: Muy cerca de 5,000 caras (50%)
- ∞ lanzamientos: Exactamente 50%

### El Teorema del Límite Central (TLC)

El TLC complementa la LGN al describir **cómo** converge el promedio:

$$\frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}} \xrightarrow{d} N(0,1)$$

**Interpretación:** El promedio de $n$ variables aleatorias se distribuye aproximadamente como una Normal, sin importar la distribución original de las variables.

**Implicación Práctica:**

Podemos construir intervalos de confianza para nuestras estimaciones:

$$\bar{X}_n \pm z_{\alpha/2} \frac{\sigma}{\sqrt{n}}$$

**Ejemplo:**

```python
import numpy as np
from scipy import stats

# Simular lanzamientos de dado (distribución uniforme discreta)
n_sim = 1000
lanzamientos = np.random.randint(1, 7, size=n_sim)

# Promedio
media_sim = np.mean(lanzamientos)
media_teorica = 3.5

# Error estándar
sigma = np.std(lanzamientos, ddof=1)
error_std = sigma / np.sqrt(n_sim)

# Intervalo de confianza 95%
z_crit = stats.norm.ppf(0.975)
ic_inf = media_sim - z_crit * error_std
ic_sup = media_sim + z_crit * error_std

print(f"Media simulada: {media_sim:.4f}")
print(f"Media teórica: {media_teorica:.4f}")
print(f"IC 95%: [{ic_inf:.4f}, {ic_sup:.4f}]")
print(f"¿Contiene la media teórica? {ic_inf <= media_teorica <= ic_sup}")
```

---

## Características Fundamentales

### 1. Convergencia Estocástica

**Tasa de Convergencia:** $O(1/\sqrt{n})$

El error de estimación disminuye proporcionalmente a la raíz cuadrada del número de simulaciones:

$$\text{Error} \approx \frac{\sigma}{\sqrt{n}}$$

**Implicación:**
- Para ganar 1 dígito decimal de precisión, necesitas 100× más simulaciones
- Para reducir el error a la mitad, necesitas 4× más simulaciones

**Ejemplo Numérico:**

```python
def estimar_pi_con_error(n):
    """Estima π y calcula el error"""
    dentro = 0
    for _ in range(n):
        x, y = np.random.random(2)
        if x**2 + y**2 <= 1:
            dentro += 1
    
    pi_est = 4 * dentro / n
    error = abs(pi_est - np.pi)
    
    return pi_est, error

# Demostrar convergencia
print("n         π estimado    Error")
print("-" * 40)
for n in [100, 1000, 10000, 100000, 1000000]:
    pi_est, error = estimar_pi_con_error(n)
    print(f"{n:>7}   {pi_est:.6f}     {error:.6f}")

# Salida típica:
#     100   3.120000     0.021593
#    1000   3.136000     0.005593
#   10000   3.141200     0.000393
#  100000   3.141472     0.000121
# 1000000   3.141621     0.000028
```

**Observación:** El error se reduce aproximadamente por un factor de $\sqrt{10} \approx 3.16$ cada vez que multiplicamos $n$ por 10.

### 2. Independencia de la Dimensión

**Ventaja Clave:** La tasa de convergencia $O(1/\sqrt{n})$ es **independiente de la dimensión** del problema.

**Comparación con Métodos Numéricos Tradicionales:**

| Método | Tasa de Convergencia | Dependencia de Dimensión |
|--------|----------------------|--------------------------|
| **Regla del Trapecio** | $O(1/n^2)$ | $O(n^d)$ puntos necesarios |
| **Simpson** | $O(1/n^4)$ | $O(n^d)$ puntos necesarios |
| **Monte Carlo** | $O(1/\sqrt{n})$ | $O(n)$ puntos (independiente de $d$) |

**Ejemplo: Integración en Alta Dimensión**

Para integrar en un cubo de dimensión $d$ con precisión $\epsilon$:

- **Trapecio:** Necesitas $n^d$ puntos, donde $n \sim 1/\epsilon^{1/2}$
  - $d=2$: $n^2$ puntos
  - $d=10$: $n^{10}$ puntos (¡intratable!)

- **Monte Carlo:** Necesitas $n \sim 1/\epsilon^2$ puntos
  - $d=2$: $n$ puntos
  - $d=10$: $n$ puntos (¡mismo costo!)

```python
def integral_alta_dimension(d, n):
    """
    Calcula el volumen de una hiperesfera de dimensión d
    inscrita en un hipercubo [-1,1]^d
    """
    dentro = 0
    
    for _ in range(n):
        punto = np.random.uniform(-1, 1, d)
        if np.sum(punto**2) <= 1:
            dentro += 1
    
    volumen_cubo = 2**d
    volumen_esfera = volumen_cubo * dentro / n
    
    return volumen_esfera

# Comparar diferentes dimensiones
print("Dimensión    Volumen Estimado    Tiempo (relativo)")
print("-" * 55)

import time

for d in [2, 5, 10, 20]:
    inicio = time.time()
    vol = integral_alta_dimension(d, n=100000)
    tiempo = time.time() - inicio
    
    print(f"{d:>5}        {vol:.6f}            {tiempo:.4f}s")

# Observación: El tiempo es casi constante independientemente de d
```

### 3. Flexibilidad y Generalidad

**Ventaja:** Monte Carlo puede aplicarse a problemas donde los métodos analíticos o numéricos tradicionales fallan:

- Integrales de alta dimensión
- Sistemas con geometrías complejas
- Procesos estocásticos
- Optimización combinatoria
- Ecuaciones diferenciales parciales

**Ejemplo: Geometría Compleja**

Calcular el área de una figura irregular:

```python
def area_figura_irregular(n):
    """
    Calcula el área de una figura definida implícitamente:
    x^2 + y^2 < 1 AND x*y > 0.1
    """
    dentro = 0
    
    for _ in range(n):
        x = np.random.uniform(-1, 1)
        y = np.random.uniform(-1, 1)
        
        if x**2 + y**2 < 1 and x*y > 0.1:
            dentro += 1
    
    area_cuadrado = 4  # [-1,1] × [-1,1]
    area_figura = area_cuadrado * dentro / n
    
    return area_figura

area = area_figura_irregular(100000)
print(f"Área de la figura irregular: {area:.4f}")
```

---

## Varianza y Reducción de Varianza

### Varianza de la Estimación

La precisión de una estimación Monte Carlo depende de la varianza de la variable aleatoria:

$$\text{Var}(\bar{X}_n) = \frac{\sigma^2}{n}$$

**Implicación:** Para mejorar la precisión, podemos:
1. Aumentar $n$ (costoso)
2. Reducir $\sigma^2$ (inteligente)

### Técnicas de Reducción de Varianza

#### 1. Variables Antitéticas

**Idea:** Si $U \sim U[0,1]$, entonces $1-U \sim U[0,1]$ también. Usar ambos reduce la varianza.

```python
def estimar_integral_antiteticas(f, a, b, n):
    """
    Estima integral usando variables antitéticas
    """
    suma = 0
    
    for _ in range(n//2):
        u = np.random.random()
        
        # Par antitético
        x1 = a + (b-a) * u
        x2 = a + (b-a) * (1-u)
        
        suma += f(x1) + f(x2)
    
    return (b - a) * suma / n

# Comparar varianza
f = lambda x: np.exp(-x**2)

# Método estándar
resultados_std = [integral_monte_carlo(f, 0, 1, 1000) for _ in range(100)]
var_std = np.var(resultados_std)

# Método antitético
resultados_ant = [estimar_integral_antiteticas(f, 0, 1, 1000) for _ in range(100)]
var_ant = np.var(resultados_ant)

print(f"Varianza estándar: {var_std:.6f}")
print(f"Varianza antitética: {var_ant:.6f}")
print(f"Reducción: {(1 - var_ant/var_std)*100:.1f}%")
```

#### 2. Muestreo por Importancia

**Idea:** Muestrear más frecuentemente en regiones "importantes" de la función.

**Ejemplo:** Para estimar $\int_0^\infty e^{-x^2} dx$, muestrear más cerca de $x=0$ donde la función es mayor.

#### 3. Variables de Control

**Idea:** Usar una variable correlacionada con resultado conocido para corregir la estimación.

---

## Aplicaciones Clásicas

### 1. Estimación de π

Ya vimos el método del círculo. Aquí una variante usando la serie de Leibniz:

$$\frac{\pi}{4} = \sum_{k=0}^{\infty} \frac{(-1)^k}{2k+1}$$

```python
def estimar_pi_leibniz_mc(n):
    """
    Estima π usando Monte Carlo sobre la serie de Leibniz
    """
    suma = 0
    
    for _ in range(n):
        k = np.random.randint(0, 1000)  # Truncar serie
        suma += (-1)**k / (2*k + 1)
    
    return 4 * suma / n

pi_est = estimar_pi_leibniz_mc(100000)
print(f"π ≈ {pi_est:.5f}")
```

### 2. Integración Numérica

**Problema:** Calcular $\int_0^1 e^{-x^2} dx$

**Método Monte Carlo:**

$$I \approx \frac{1}{n}\sum_{i=1}^{n} f(x_i), \quad x_i \sim U[0,1]$$

```python
def integral_monte_carlo(f, a, b, n):
    """Calcula integral de f en [a,b] usando Monte Carlo"""
    x = np.random.uniform(a, b, n)
    return (b - a) * np.mean(f(x))

# Ejemplo
f = lambda x: np.exp(-x**2)
resultado = integral_monte_carlo(f, 0, 1, 100000)
print(f"∫₀¹ e^(-x²) dx ≈ {resultado:.5f}")

# Valor real (usando scipy)
from scipy.integrate import quad
real, _ = quad(f, 0, 1)
print(f"Valor real: {real:.5f}")
print(f"Error: {abs(resultado - real):.5f}")
```

### 3. Simulación de Procesos Estocásticos

**Problema:** Simular el movimiento Browniano (Wiener process)

$$W(t) = W(0) + \sum_{i=1}^{n} \sqrt{\Delta t} \cdot Z_i, \quad Z_i \sim N(0,1)$$

```python
def simular_browniano(T, n_pasos):
    """
    Simula un movimiento Browniano en [0, T]
    """
    dt = T / n_pasos
    t = np.linspace(0, T, n_pasos+1)
    
    # Incrementos aleatorios
    dW = np.random.normal(0, np.sqrt(dt), n_pasos)
    
    # Trayectoria acumulada
    W = np.concatenate([[0], np.cumsum(dW)])
    
    return t, W

# Simular y graficar
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))

for _ in range(5):
    t, W = simular_browniano(T=1, n_pasos=1000)
    plt.plot(t, W, alpha=0.7)

plt.xlabel('Tiempo')
plt.ylabel('W(t)')
plt.title('Movimiento Browniano (5 trayectorias)')
plt.grid(True, alpha=0.3)
plt.show()
```

---

## Análisis de Convergencia

### Estudio Empírico de la Convergencia

```python
def estudiar_convergencia():
    """
    Estudia empíricamente la convergencia de Monte Carlo
    """
    valores_n = [100, 500, 1000, 5000, 10000, 50000, 100000]
    errores = []
    
    # Función a integrar
    f = lambda x: np.exp(-x**2)
    valor_real = 0.746824  # Valor conocido
    
    for n in valores_n:
        # Repetir 50 veces para obtener error promedio
        errores_n = []
        for _ in range(50):
            estimacion = integral_monte_carlo(f, 0, 1, n)
            error = abs(estimacion - valor_real)
            errores_n.append(error)
        
        error_promedio = np.mean(errores_n)
        errores.append(error_promedio)
    
    # Graficar en escala log-log
    plt.figure(figsize=(10, 6))
    plt.loglog(valores_n, errores, 'o-', label='Error observado')
    
    # Línea teórica O(1/√n)
    teorica = [0.1 / np.sqrt(n) for n in valores_n]
    plt.loglog(valores_n, teorica, '--', label='O(1/√n) teórico')
    
    plt.xlabel('Número de simulaciones (n)')
    plt.ylabel('Error promedio')
    plt.title('Convergencia del Método de Monte Carlo')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

estudiar_convergencia()
```

---

## Ejercicios Prácticos

### Ejercicio 1: Verificar la LGN

Simule el lanzamiento de un dado 10,000 veces y grafique la convergencia del promedio hacia 3.5.

**Solución:**

```python
n_max = 10000
lanzamientos = np.random.randint(1, 7, n_max)

# Promedios acumulados
promedios = np.cumsum(lanzamientos) / np.arange(1, n_max+1)

plt.figure(figsize=(12, 6))
plt.plot(promedios, alpha=0.7)
plt.axhline(y=3.5, color='r', linestyle='--', label='Valor esperado (3.5)')
plt.xlabel('Número de lanzamientos')
plt.ylabel('Promedio acumulado')
plt.title('Ley de los Grandes Números: Lanzamiento de Dado')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### Ejercicio 2: Comparar Métodos de Integración

Compare el error y tiempo de ejecución de:
1. Regla del trapecio
2. Monte Carlo

Para calcular $\int_0^1 \sin(x) dx = 1 - \cos(1) \approx 0.4597$

### Ejercicio 3: Reducción de Varianza

Implemente el método de variables antitéticas para estimar $\int_0^1 x^2 dx = 1/3$ y compare la varianza con el método estándar.

---

## Casos de Estudio

### Caso 1: Estimación de Área Bajo Curva Compleja

**Problema:** Calcular el área bajo la curva $y = \sin(x) + 0.5\sin(3x)$ en $[0, 2\pi]$

```python
def area_curva_compleja(n):
    """Estima área usando Monte Carlo"""
    f = lambda x: np.sin(x) + 0.5*np.sin(3*x)
    
    # Encontrar rango de y
    x_test = np.linspace(0, 2*np.pi, 1000)
    y_max = np.max(f(x_test))
    y_min = np.min(f(x_test))
    
    dentro = 0
    
    for _ in range(n):
        x = np.random.uniform(0, 2*np.pi)
        y = np.random.uniform(y_min, y_max)
        
        if y_min <= y <= f(x):
            dentro += 1
    
    area_rectangulo = 2*np.pi * (y_max - y_min)
    area_curva = area_rectangulo * dentro / n
    
    return area_curva

area = area_curva_compleja(100000)
print(f"Área estimada: {area:.4f}")
```

### Caso 2: Simulación de Camino Aleatorio

**Problema:** Un borracho camina aleatoriamente. ¿Cuál es la distancia esperada del origen después de $n$ pasos?

```python
def camino_aleatorio_2d(n_pasos, n_sim):
    """
    Simula camino aleatorio en 2D
    """
    distancias_finales = []
    
    for _ in range(n_sim):
        x, y = 0, 0
        
        for _ in range(n_pasos):
            angulo = np.random.uniform(0, 2*np.pi)
            x += np.cos(angulo)
            y += np.sin(angulo)
        
        distancia = np.sqrt(x**2 + y**2)
        distancias_finales.append(distancia)
    
    return np.mean(distancias_finales), np.std(distancias_finales)

# Teoría: E[distancia] ≈ √n para camino aleatorio
for n in [10, 100, 1000]:
    media, std = camino_aleatorio_2d(n, n_sim=1000)
    teorico = np.sqrt(n)
    print(f"n={n:>4}: Simulado={media:.2f}, Teórico={teorico:.2f}, Desv={std:.2f}")
```

---

## Limitaciones y Consideraciones

### 1. Convergencia Lenta

**Problema:** $O(1/\sqrt{n})$ es lento comparado con métodos determinísticos en baja dimensión.

**Solución:** Usar Monte Carlo solo cuando:
- Dimensión alta ($d > 4$)
- Geometría compleja
- No hay alternativa analítica

### 2. Dependencia de la Calidad del RNG

**Problema:** Un generador de números pseudoaleatorios defectuoso puede dar resultados completamente erróneos.

**Solución:** Usar generadores validados (Mersenne Twister, PCG, Xorshift)

### 3. Estimación de la Varianza

**Problema:** Para construir intervalos de confianza, necesitamos estimar $\sigma^2$.

**Solución:** Usar la varianza muestral:

$$s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})^2$$

---

## Resumen y Mejores Prácticas

**Conceptos Clave:**
- LGN: Convergencia del promedio al valor esperado
- TLC: Distribución normal del promedio
- Convergencia: $O(1/\sqrt{n})$, independiente de dimensión
- Reducción de varianza: Variables antitéticas, muestreo por importancia

**Recomendaciones:**
1. Usar Monte Carlo para problemas de alta dimensión
2. Validar convergencia con múltiples corridas
3. Reportar intervalos de confianza, no solo estimaciones puntuales
4. Considerar técnicas de reducción de varianza para problemas costosos

---

*Referencia: Programa SCD-1022 - TecNM*
*Fuentes: Rubinstein & Kroese (2016), Robert & Casella (2004)*


---

<div align="center">

⬅️ [2.3 Monte Carlo](2.3.md) &nbsp;&nbsp;|&nbsp;&nbsp; [2.3.2 Aplicaciones](2.3.2.md) ➡️

</div>
