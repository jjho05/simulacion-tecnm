# 2.3 Método de Monte Carlo y Generación de Variables Aleatorias

En las secciones anteriores aprendimos a generar números pseudoaleatorios uniformes ($r_i \sim U[0,1]$). Sin embargo, el mundo real rara vez es uniforme. Los tiempos de servicio suelen ser exponenciales, las estaturas siguen una distribución normal, y el número de clientes por hora puede ser Poisson.

Esta sección es el "corazón" de la simulación: **¿Cómo transformamos un simple número uniforme $U[0,1]$ en cualquier distribución de probabilidad compleja?**

---

## El Concepto Fundamental

La generación de variables no uniformes (V.A.) se basa en tomar una fuente de aleatoriedad uniforme (nuestra "materia prima") y aplicarle una transformación matemática o algorítmica (nuestra "maquinaria") para moldearla en la distribución deseada.

$$R \sim U[0,1] \xrightarrow{\text{Algoritmo}} X \sim f(x)$$

### Ejemplo Conceptual

Imagina que tienes una máquina que genera números del 1 al 100 con igual probabilidad (uniforme). Quieres simular el lanzamiento de una moneda (50% cara, 50% cruz):

```python
# Transformación simple
r = random.randint(1, 100)  # Uniforme [1, 100]
if r <= 50:
    resultado = "Cara"
else:
    resultado = "Cruz"
```

Este es el principio básico: **transformar uniformidad en cualquier distribución**.

---

## Métodos Generales de Generación

Existen cuatro grandes familias de métodos para generar variables aleatorias, cada uno con sus ventajas y desventajas:

### 1. Método de la Transformada Inversa

El método más universal y elegante teóricamente.

**Base Matemática:**
Si $X$ tiene una función de distribución acumulada (CDF) $F(x)$, entonces la variable aleatoria $F(X)$ tiene una distribución uniforme en $[0,1]$.

**Algoritmo:**
1. Generar $R \sim U[0,1]$
2. Calcular $X = F^{-1}(R)$

**Ejemplo: Distribución Exponencial**

Para $X \sim \text{Exp}(\lambda)$:
- CDF: $F(x) = 1 - e^{-\lambda x}$
- Inversa: $F^{-1}(r) = -\frac{1}{\lambda}\ln(1-r)$

```python
import numpy as np

def generar_exponencial(lambd, n=1):
    """Genera n variables exponenciales con parámetro lambda"""
    r = np.random.random(n)
    return -np.log(1 - r) / lambd

# Ejemplo: Tiempos entre llegadas (λ=2 clientes/hora)
tiempos = generar_exponencial(lambd=2, n=1000)
print(f"Media teórica: {1/2:.2f}, Media simulada: {np.mean(tiempos):.2f}")
```

**Ventajas:**
- Matemáticamente elegante
- Exacto (no aproximado)
- Eficiente computacionalmente

**Limitaciones:**
- Requiere que $F^{-1}$ tenga forma cerrada
- No funciona para Normal, Gamma, Beta (sin forma cerrada simple)

### 2. Método de Aceptación-Rechazo

El método "fuerza bruta" inteligente.

**Concepto Visual:**

Imagina que quieres generar puntos bajo una curva $f(x)$ compleja. En lugar de calcular la inversa, simplemente:
1. Generas puntos aleatorios en un rectángulo que contiene la curva
2. Aceptas los que caen bajo la curva
3. Rechazas los que caen arriba

**Algoritmo:**
1. Encontrar una función "envoltura" $g(x)$ y constante $c$ tal que $f(x) \leq c \cdot g(x)$ para todo $x$
2. Generar $Y$ de $g(y)$
3. Generar $U \sim U[0,1]$
4. Si $U \leq \frac{f(Y)}{c \cdot g(Y)}$, ACEPTAR $X=Y$
5. Si no, RECHAZAR y volver al paso 2

**Ejemplo: Distribución Beta(2,2)**

```python
def generar_beta_22(n=1):
    """Genera Beta(2,2) usando aceptación-rechazo"""
    resultados = []
    rechazos = 0
    
    while len(resultados) < n:
        # Generar candidato de U[0,1]
        y = np.random.random()
        u = np.random.random()
        
        # f(y) = 6y(1-y) para Beta(2,2)
        # g(y) = 1 para U[0,1]
        # c = 1.5 (máximo de f)
        
        if u <= 6*y*(1-y) / 1.5:
            resultados.append(y)
        else:
            rechazos += 1
    
    print(f"Tasa de aceptación: {n/(n+rechazos)*100:.1f}%")
    return np.array(resultados)

# Ejemplo
muestras = generar_beta_22(1000)
print(f"Media teórica: 0.5, Media simulada: {np.mean(muestras):.3f}")
```

**Ventajas:**
- Funciona para CUALQUIER densidad acotada
- No requiere inversa analítica

**Limitaciones:**
- Ineficiente si $c$ es grande (muchos rechazos)
- Requiere encontrar una buena función envoltura

### 3. Método de Composición

El método "divide y vencerás".

**Concepto:**
Si una distribución compleja puede verse como la suma ponderada de distribuciones más simples:

$$f(x) = \sum_{i=1}^{k} p_i f_i(x)$$

**Algoritmo:**
1. Generar $U \sim U[0,1]$
2. Seleccionar sub-distribución $i$ tal que $\sum_{j=1}^{i-1} p_j < U \leq \sum_{j=1}^{i} p_j$
3. Generar $X$ de $f_i(x)$

**Ejemplo: Distribución Hiperexponencial**

Mezcla de dos exponenciales (útil para modelar tiempos de servicio con alta variabilidad):

```python
def generar_hiperexponencial(p1, lambda1, lambda2, n=1):
    """
    Genera n variables de una hiperexponencial.
    Con probabilidad p1: Exp(lambda1)
    Con probabilidad 1-p1: Exp(lambda2)
    """
    resultados = []
    
    for _ in range(n):
        u = np.random.random()
        
        if u < p1:
            # Usar primera exponencial
            x = -np.log(np.random.random()) / lambda1
        else:
            # Usar segunda exponencial
            x = -np.log(np.random.random()) / lambda2
        
        resultados.append(x)
    
    return np.array(resultados)

# Ejemplo: 70% clientes rápidos (λ=5), 30% clientes lentos (λ=0.5)
tiempos = generar_hiperexponencial(p1=0.7, lambda1=5, lambda2=0.5, n=1000)
print(f"Media: {np.mean(tiempos):.3f}, Desv.Est: {np.std(tiempos):.3f}")
```

**Ventajas:**
- Muy intuitivo
- Eficiente si las sub-distribuciones son simples

**Limitaciones:**
- Requiere descomponer la distribución objetivo

### 4. Método de Convolución

El método de la "suma de variables".

**Concepto:**
Muchas distribuciones importantes son la suma de otras variables aleatorias más simples:
- Erlang-$k$ = suma de $k$ exponenciales
- Binomial = suma de $n$ Bernoulli
- Normal ≈ suma de 12 uniformes (Teorema del Límite Central)

**Ejemplo: Distribución Erlang**

```python
def generar_erlang(k, lambd, n=1):
    """
    Genera n variables Erlang(k, lambda).
    Erlang = suma de k exponenciales independientes.
    """
    resultados = []
    
    for _ in range(n):
        # Sumar k exponenciales
        suma = 0
        for _ in range(k):
            r = np.random.random()
            suma += -np.log(r) / lambd
        
        resultados.append(suma)
    
    return np.array(resultados)

# Ejemplo: Tiempo de procesamiento de 3 etapas
tiempos = generar_erlang(k=3, lambd=2, n=1000)
print(f"Media teórica: {3/2:.2f}, Media simulada: {np.mean(tiempos):.2f}")
```

**Ventajas:**
- Muy intuitivo físicamente
- Fácil de implementar

**Limitaciones:**
- Lento si $k$ es grande
- Solo funciona para distribuciones que son sumas

---

## Introducción al Método de Monte Carlo

El término "Monte Carlo" fue acuñado por Stanislaw Ulam y John von Neumann en el Laboratorio de Los Alamos (años 40) mientras trabajaban en armas nucleares. Necesitaban resolver integrales de difusión de neutrones imposibles de atacar analíticamente.

### ¿Qué es Monte Carlo?

Es una clase amplia de algoritmos computacionales que utilizan el **muestreo aleatorio repetido** para obtener resultados numéricos.

> **Paradoja:** Usamos la aleatoriedad para resolver problemas que podrían ser completamente determinísticos (como calcular una integral definida o el valor de $\pi$).

### ¿Por qué funciona?

Se basa en la **Ley de los Grandes Números**: si realizamos un experimento aleatorio infinitas veces, el promedio de los resultados convergerá al valor esperado (teórico).

$$\lim_{N \to \infty} \frac{1}{N}\sum_{i=1}^{N} X_i = E[X]$$

**Propiedades Clave:**

1. **Convergencia:** $O(1/\sqrt{N})$
   - Para ganar 1 dígito decimal de precisión, necesitamos 100× más simulaciones

2. **Independencia de Dimensión:**
   - Integrar en 100 dimensiones es casi tan fácil como en 1 dimensión
   - Métodos numéricos tradicionales (trapecio, Simpson) colapsan en alta dimensión

### Ejemplo Clásico: Estimación de π

```python
def estimar_pi(n):
    """
    Estima π usando Monte Carlo.
    Lanza puntos aleatorios en un cuadrado [0,1]×[0,1]
    y cuenta cuántos caen dentro del círculo de radio 1.
    """
    dentro_circulo = 0
    
    for _ in range(n):
        x = np.random.random()
        y = np.random.random()
        
        if x**2 + y**2 <= 1:
            dentro_circulo += 1
    
    # Área del círculo / Área del cuadrado = π/4
    pi_estimado = 4 * dentro_circulo / n
    
    return pi_estimado

# Convergencia
for n in [100, 1000, 10000, 100000, 1000000]:
    pi_est = estimar_pi(n)
    error = abs(pi_est - np.pi)
    print(f"n={n:>7}: π≈{pi_est:.5f}, Error={error:.5f}")

# Salida esperada:
# n=    100: π≈3.16000, Error=0.01841
# n=   1000: π≈3.14800, Error=0.00641
# n=  10000: π≈3.14320, Error=0.00161
# n= 100000: π≈3.14207, Error=0.00048
# n=1000000: π≈3.14153, Error=0.00006
```

---

## Aplicaciones del Método de Monte Carlo

### 1. Integración Numérica

Calcular $I = \int_a^b f(x)dx$ cuando $f(x)$ es muy compleja o de alta dimensión.

**Método:**
$$I \approx \frac{b-a}{N}\sum_{i=1}^{N} f(x_i), \quad x_i \sim U[a,b]$$

```python
def integral_monte_carlo(f, a, b, n):
    """Calcula integral de f en [a,b] usando Monte Carlo"""
    x = np.random.uniform(a, b, n)
    return (b - a) * np.mean(f(x))

# Ejemplo: ∫₀¹ e^(-x²) dx
f = lambda x: np.exp(-x**2)
resultado = integral_monte_carlo(f, 0, 1, 100000)
print(f"Integral ≈ {resultado:.5f}")
# Valor real ≈ 0.74682
```

### 2. Finanzas: Valoración de Opciones

Calcular el precio de una opción europea usando Black-Scholes.

```python
def precio_opcion_call_mc(S0, K, r, sigma, T, n_sim):
    """
    S0: Precio inicial
    K: Strike
    r: Tasa libre de riesgo
    sigma: Volatilidad
    T: Tiempo hasta vencimiento
    """
    # Simular precio final del activo
    Z = np.random.standard_normal(n_sim)
    ST = S0 * np.exp((r - 0.5*sigma**2)*T + sigma*np.sqrt(T)*Z)
    
    # Payoff de la opción call
    payoffs = np.maximum(ST - K, 0)
    
    # Descontar al presente
    precio = np.exp(-r*T) * np.mean(payoffs)
    
    return precio

# Ejemplo
precio = precio_opcion_call_mc(S0=100, K=105, r=0.05, sigma=0.2, T=1, n_sim=100000)
print(f"Precio de la opción: ${precio:.2f}")
```

### 3. Física: Transporte de Partículas

Simular el recorrido de neutrones en un reactor nuclear.

### 4. Ingeniería: Análisis de Confiabilidad

Estimar la probabilidad de falla de un sistema complejo.

---

## Comparación de Métodos

| Método | Ventajas | Desventajas | Mejor Para |
|--------|----------|-------------|------------|
| **Transformada Inversa** | Exacto, rápido | Requiere $F^{-1}$ | Exp, Weibull, Uniforme |
| **Aceptación-Rechazo** | Universal | Ineficiente si mala envoltura | Beta, Gamma |
| **Composición** | Intuitivo | Requiere descomposición | Hiperexponencial, Mixtas |
| **Convolución** | Físicamente natural | Lento para $k$ grande | Erlang, Binomial |

---

## Ejercicios Prácticos

### Ejercicio 1: Transformada Inversa

Genere 1000 variables de una distribución Weibull con parámetros $\alpha=2$, $\beta=1$.

**CDF:** $F(x) = 1 - e^{-(x/\beta)^\alpha}$

**Inversa:** $F^{-1}(r) = \beta(-\ln(1-r))^{1/\alpha}$

**Solución:**

```python
def generar_weibull(alpha, beta, n):
    r = np.random.random(n)
    return beta * (-np.log(1 - r))**(1/alpha)

muestras = generar_weibull(alpha=2, beta=1, n=1000)
print(f"Media: {np.mean(muestras):.3f}")
print(f"Desv.Est: {np.std(muestras):.3f}")
```

### Ejercicio 2: Monte Carlo para Integral

Calcule $\int_0^1 \sqrt{1-x^2} dx$ (área de un cuarto de círculo).

**Valor real:** $\pi/4 \approx 0.7854$

### Ejercicio 3: Comparación de Eficiencia

Compare el tiempo de ejecución de generar 10,000 variables Erlang(10, 1) usando:
1. Convolución (sumando 10 exponenciales)
2. Transformada inversa (si encuentra la fórmula)

---

## Mapa Conceptual de la Sección 2.3

Para dominar esta unidad, profundizaremos en los siguientes temas:

1. **[2.3.1] Características del Método:** Fundamentos estocásticos, convergencia y propiedades matemáticas.
2. **[2.3.2] Aplicaciones:** Desde finanzas (valoración Black-Scholes) hasta física (transporte de partículas) e ingeniería.
3. **[2.3.3] Solución de Problemas:** Metodología paso a paso para plantear y resolver simulaciones completas.

Esta sección es el puente entre la teoría estadística (Unidad 1 y 2.1-2.2) y la construcción de simuladores complejos (Unidad 3 y sucesivas).

---

## Casos de Estudio

### Caso 1: Simulación de Inventario

**Problema:** Una tienda vende un producto con demanda aleatoria. ¿Cuánto inventario mantener?

**Modelo:**
- Demanda diaria: $D \sim \text{Poisson}(\lambda=10)$
- Costo de almacenamiento: $h = \$2$/unidad/día
- Costo de faltante: $p = \$50$/unidad
- Política: Revisar cada semana, pedir hasta nivel $S$

**Simulación:**

```python
def simular_inventario(S, n_semanas):
    inventario = S
    costo_total = 0
    
    for semana in range(n_semanas):
        for dia in range(7):
            # Generar demanda (Poisson)
            demanda = np.random.poisson(10)
            
            if inventario >= demanda:
                inventario -= demanda
                costo_total += 2 * inventario  # Almacenamiento
            else:
                faltante = demanda - inventario
                inventario = 0
                costo_total += 50 * faltante  # Faltante
        
        # Revisar y pedir
        inventario = S
    
    return costo_total / n_semanas

# Probar diferentes políticas
for S in [50, 70, 90, 110]:
    costo = simular_inventario(S, n_semanas=1000)
    print(f"S={S}: Costo promedio semanal = ${costo:.2f}")
```

### Caso 2: Estimación de Riesgo Financiero (VaR)

**Problema:** Calcular el Value at Risk (VaR) de un portafolio.

**Modelo:**
- Portafolio: 60% acciones, 40% bonos
- Retornos: $R_a \sim N(0.08, 0.15)$, $R_b \sim N(0.04, 0.05)$
- Correlación: $\rho = 0.3$

```python
def calcular_var(n_sim, confianza=0.95):
    # Generar retornos correlacionados
    retornos = np.random.multivariate_normal(
        mean=[0.08, 0.04],
        cov=[[0.15**2, 0.3*0.15*0.05],
             [0.3*0.15*0.05, 0.05**2]],
        size=n_sim
    )
    
    # Retorno del portafolio
    ret_portafolio = 0.6 * retornos[:, 0] + 0.4 * retornos[:, 1]
    
    # VaR (percentil)
    var = np.percentile(ret_portafolio, (1-confianza)*100)
    
    return var

var_95 = calcular_var(100000, confianza=0.95)
print(f"VaR al 95%: {var_95:.4f} ({var_95*100:.2f}%)")
```

---

## Errores Comunes

### Error 1: Confundir Método con Aplicación

❌ **Incorrecto:** "Monte Carlo es para calcular π"
✅ **Correcto:** Monte Carlo es una familia de métodos que usan muestreo aleatorio

### Error 2: Ignorar la Convergencia

❌ **Incorrecto:** Usar n=100 y esperar 4 decimales de precisión
✅ **Correcto:** Error $\propto 1/\sqrt{n}$, necesitas n=10,000 para 2 decimales

### Error 3: No Validar el Generador

❌ **Incorrecto:** Implementar un método y asumir que funciona
✅ **Correcto:** Validar media, varianza, histograma contra valores teóricos

---

## Resumen y Mejores Prácticas

**Conceptos Clave:**
- Transformación: $U[0,1] \to$ cualquier distribución
- Cuatro métodos principales: Inversa, Aceptación-Rechazo, Composición, Convolución
- Monte Carlo: Muestreo aleatorio para problemas numéricos
- Convergencia: $O(1/\sqrt{N})$

**Recomendaciones:**
1. Elegir el método según la distribución objetivo
2. Validar siempre con estadísticas teóricas
3. Usar semillas para reproducibilidad
4. Documentar parámetros y supuestos

---

*Referencia: Programa SCD-1022 - TecNM*
*Fuentes: Law & Kelton, "Simulation Modeling and Analysis"; Rubinstein, "Simulation and the Monte Carlo Method"*


---

<div align="center">

⬅️ [2.2.3 Independencia](2.2.3.md) &nbsp;&nbsp;|&nbsp;&nbsp; [2.3.1 Características](2.3.1.md) ➡️

</div>
